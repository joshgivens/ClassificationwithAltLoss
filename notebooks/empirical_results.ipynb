{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "#\n",
    "# The copyright of this file belongs to Feedzai. The file cannot be\n",
    "# reproduced in whole or in part, stored in a retrieval system,\n",
    "# transmitted in any form, or by any means electronic, mechanical,\n",
    "# photocopying, or otherwise, without the prior permission of the owner.\n",
    "#\n",
    "# (c) 2022 Feedzai, Strictly Confidential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm  # Tested ML method\n",
    "import numpy as np       # Random number generation\n",
    "import seaborn as sns    # Plotting library\n",
    "import pandas as pd      # Read/write data\n",
    "import yaml              # Read hyperparameter space configuration\n",
    "\n",
    "# from aequitas.group import Group                # Fairness metrics\n",
    "from matplotlib import pyplot as plt            # Plotting method\n",
    "from sklearn.preprocessing import LabelEncoder  # Categorical encoding for LGBM models\n",
    "from sklearn import metrics                     # ROC metrics\n",
    "\n",
    "from random_search import RandomValueTrial, suggest_callable_hyperparams  # Random search wrapper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read hyperparameter space for the LGBM Models, expected structure is presented bellow\n",
    "with open(\"lightgbm_hyperparameter_space.yaml\", \"r\") as file:\n",
    "    hyperparam_space = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LightGBM': {'classpath': 'lightgbm.LGBMClassifier',\n",
       "  'kwargs': {'n_estimators': {'type': 'int',\n",
       "    'range': [20, 10000],\n",
       "    'log': True},\n",
       "   'max_depth': {'type': 'int', 'range': [3, 30]},\n",
       "   'learning_rate': {'type': 'float', 'range': [0.02, 0.1], 'log': True},\n",
       "   'num_leaves': {'type': 'int', 'range': [10, 100], 'log': True},\n",
       "   'boosting_type': ['gbdt', 'goss'],\n",
       "   'min_data_in_leaf': {'type': 'int', 'range': [5, 200], 'log': True},\n",
       "   'max_bin': {'type': 'int', 'range': [100, 500]},\n",
       "   'enable_bundle': [True, False]}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The expected structure is the following:\n",
    "hyperparam_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classpath': 'lightgbm.LGBMClassifier',\n",
       " 'n_estimators': 263,\n",
       " 'max_depth': 23,\n",
       " 'learning_rate': 0.020003681922217444,\n",
       " 'num_leaves': 19,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'min_data_in_leaf': 9,\n",
       " 'max_bin': 238,\n",
       " 'enable_bundle': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing a random search suggestion:\n",
    "trial = RandomValueTrial(seed=1)\n",
    "suggest_callable_hyperparams(trial, hyperparam_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define path to datasets. Replace `base_path` with the appropriate value.\n",
    "base_path = \"../archive/\"\n",
    "\n",
    "datasets_paths = {\n",
    "    \"Base\":    base_path + \"Base.parquet\",\n",
    "    \"TypeI\":   base_path + \"TypeI.parquet\",\n",
    "    \"TypeII\":  base_path + \"TypeII.parquet\",\n",
    "    \"TypeIII\": base_path + \"TypeIII.parquet\",\n",
    "    \"TypeIV\":  base_path + \"TypeIV.parquet\",\n",
    "    \"TypeV\":   base_path + \"TypeV.parquet\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'archive/Base.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read the datasets with pandas.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {key: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, path \u001b[38;5;129;01min\u001b[39;00m datasets_paths\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\songa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\songa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\songa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\songa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'archive/Base.parquet'"
     ]
    }
   ],
   "source": [
    "# Read the datasets with pandas.\n",
    "datasets = {key: pd.read_parquet(path) for key, path in datasets_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the label field and categorical columns.\n",
    "label = \"fraud_bool\"\n",
    "\n",
    "categorical_features = [\n",
    "    \"payment_type\",\n",
    "    \"employment_status\",\n",
    "    \"housing_status\",\n",
    "    \"source\",\n",
    "    \"device_os\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the train and test sets. Shuffle data with `sample` method.\n",
    "# The split was done by month. The first 6 months as the train, the last 2 months as test.\n",
    "train_dfs = {key: df[df[\"month\"]<6].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
    "test_dfs = {key: df[df[\"month\"]>=6].sample(frac=1, replace=False) for key, df in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the categorical variables in the datasets to integers. \n",
    "# This is expected by LGBM (or columns with the `categorical` data type).\n",
    "\n",
    "for name in datasets.keys():  # For each dataset in the suite\n",
    "    train = train_dfs[name]\n",
    "    test = test_dfs[name]\n",
    "\n",
    "    for feat in categorical_features:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train[feat])  # Fit an encoder to the train set.\n",
    "        train[feat] = encoder.transform(train[feat])  # Transform train set.\n",
    "        test[feat] = encoder.transform(test[feat])    # Transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m prev_hyperparams\u001b[38;5;241m.\u001b[39mappend(test_hyperparams)\n\u001b[0;32m     23\u001b[0m runs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prev_hyperparams\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():  \u001b[38;5;66;03m# Run hyperparameters on all variants of datastes.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m lgbm\u001b[38;5;241m.\u001b[39mLGBMClassifier(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtest_hyperparams)  \u001b[38;5;66;03m# Instantiate LGBM Model.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m train_dfs[dataset_name]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_bool\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell with train loop.\n",
    "\n",
    "# Define number of trials in Random search.\n",
    "n_trials=100\n",
    "# Random state for sampling seeds.\n",
    "np.random.seed(42)\n",
    "# Seeds for the random search sampling algorithm.\n",
    "seeds = np.random.choice(list(range(1_000_000)), size=n_trials, replace=False)\n",
    "\n",
    "# Variable to store the results.\n",
    "runs = {}\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    seed = seeds[trial]\n",
    "    trial = RandomValueTrial(seed=seed)\n",
    "    # Hyperparameters for the random search trial.\n",
    "    test_hyperparams = suggest_callable_hyperparams(trial, hyperparam_space)\n",
    "    del test_hyperparams[\"classpath\"] # Remove unnecessary key in hyperparaemters.\n",
    "    \n",
    "    # Update list of tested hyperparameters.\n",
    "    prev_hyperparams = runs.get(\"hyperparams\", [])\n",
    "    prev_hyperparams.append(test_hyperparams)\n",
    "    runs[\"hyperparams\"] = prev_hyperparams\n",
    "    \n",
    "    for dataset_name in datasets.keys():  # Run hyperparameters on all variants of datastes.\n",
    "        model = lgbm.LGBMClassifier(n_jobs=10, **test_hyperparams)  # Instantiate LGBM Model.\n",
    "        X_train = train_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
    "        y_train = train_dfs[dataset_name][\"fraud_bool\"]\n",
    "        X_test = test_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
    "        y_test = test_dfs[dataset_name][\"fraud_bool\"]\n",
    "        # Fit model to training data.\n",
    "        model.fit(X_train, y_train, categorical_feature=categorical_features)\n",
    "        # Obtain predictions in test data.\n",
    "        predictions = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Obtain ROC curve for the predictions.\n",
    "        fprs, tprs, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "        # Obtain threshold and recall. We select 5% FPR as threshold.\n",
    "        threshold = np.min(thresholds[fprs==max(fprs[fprs < 0.05])])\n",
    "        recall = np.max(tprs[fprs==max(fprs[fprs < 0.05])])\n",
    "\n",
    "        # Binarize predictions for Aequitas.\n",
    "        preds_binary = (predictions > threshold).astype(int)\n",
    "        \n",
    "        # Create a dataframe with protected group column, predictions and labels.\n",
    "        # Here, we select age>=50 as threshold.\n",
    "        aequitas_df = pd.DataFrame(\n",
    "            {\n",
    "                \"age\": (X_test[\"customer_age\"]>=50).map({True: \"Older\", False: \"Younger\"}),\n",
    "                \"preds\": preds_binary,\n",
    "                \"y\": y_test.values\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Obtain FPR results for different groups.\n",
    "        g = Group()\n",
    "        aequitas_results = g.get_crosstabs(aequitas_df, attr_cols=[\"age\"], score_col=\"preds\", label_col=\"y\")[0]\n",
    "        \n",
    "        # Store the results for the trained model\n",
    "        results = {}\n",
    "        results[\"recall\"] = recall\n",
    "        results[\"recall Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"tpr\"]].values[0][0]\n",
    "        results[\"recall Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"tpr\"]].values[0][0]\n",
    "        results[\"fpr Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"fpr\"]].values[0][0]\n",
    "        results[\"fpr Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"fpr\"]].values[0][0]\n",
    "        \n",
    "        # Store the results in the runs variable\n",
    "        prev_runs = runs.get(dataset_name, [])\n",
    "        prev_runs.append(results)\n",
    "        runs[dataset_name] = prev_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the results for each model in each dataset.\n",
    "rs_results = pd.DataFrame(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Helper method to obtain the metric values for a given model.\n",
    "def get_results(results, variant, metric):\n",
    "    col = results[variant]\n",
    "    values = []\n",
    "    for idx, val in col.iteritems():\n",
    "        values.append(val[metric])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain the relevant metrics to plots from the dataframe.\n",
    "variants = list(datasets_paths.keys())\n",
    "\n",
    "plot_results = {\"Variant\": [], \"Recall\": [], \"FPR Ratio\": []}\n",
    "\n",
    "for variant in variants:\n",
    "    plot_results[\"Recall\"] += get_results(rs_results, variant, \"recall\")\n",
    "    # Obtain the FPR if both groups.\n",
    "    for fpr_younger, fpr_older in zip(get_results(rs_results, variant, \"fpr Younger\"), get_results(rs_results, variant, \"fpr Older\")):\n",
    "        # Calculate FPR ratio as higher fpr / lower fpr\n",
    "        if fpr_younger > fpr_older:\n",
    "            plot_results[\"FPR Ratio\"] += [fpr_older/fpr_younger]\n",
    "        else:\n",
    "            plot_results[\"FPR Ratio\"] += [fpr_younger/fpr_older]\n",
    "    plot_results[\"Variant\"] += [variant] * len(get_results(rs_results, variant, \"recall\"))\n",
    "\n",
    "# Create a dataframe for easier plots.\n",
    "plot_results = pd.DataFrame(plot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot with the full results of the random search algorithm.\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
    "\n",
    "sns.jointplot(data=plot_results, x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\")\n",
    "plt.ylim((0,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the final plot. Highlight the top models:\n",
    "top_n = 5\n",
    "top_models_base = plot_results.loc[plot_results['Variant'] == 'Base'].sort_values('Recall', ascending=False).head(top_n).index.values\n",
    "top_models = deepcopy(top_models_base)\n",
    "for i in range(1, 6):\n",
    "    top_models = np.r_[top_models, top_models_base + (100 * i)]\n",
    "\n",
    "plot_results['index'] = plot_results.index\n",
    "plot_results['is_top'] = plot_results.apply(lambda x: 1 if x['index'] in top_models else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\", \"grid.alpha\":0.1})\n",
    "DPI = 200\n",
    "plt.rcParams['figure.dpi'] = DPI\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# LEFT PLOT\n",
    "sns.scatterplot(ax=ax1, data=plot_results.loc[(~plot_results.index.isin(top_models)), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", alpha=0.2)\n",
    "sns.scatterplot(ax=ax1, data=plot_results.loc[plot_results.index.isin(top_models), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", legend=False)\n",
    "ax1.set(\n",
    "    ylim=(0,1)\n",
    ")\n",
    "\n",
    "# RIGHT PLOT\n",
    "sns.scatterplot(ax=ax2, data=plot_results.loc[(~plot_results.index.isin(top_models)) & (plot_results[\"Variant\"].isin([\"Base\", \"Type II\", \"Type V\", \"Type IV\"])), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", alpha=0.2, palette=[sns.color_palette()[0], sns.color_palette()[2], sns.color_palette()[4], sns.color_palette()[5]], legend=False)\n",
    "sns.scatterplot(ax=ax2, data=plot_results.loc[(plot_results.index.isin(top_models)) & (plot_results[\"Variant\"].isin([\"Base\", \"Type II\", \"Type V\", \"Type IV\"])), :], x=\"Recall\", y=\"FPR Ratio\", hue=\"Variant\", palette=[sns.color_palette()[0], sns.color_palette()[2], sns.color_palette()[4], sns.color_palette()[5]], legend=False)\n",
    "ax2.set(\n",
    "    ylim=(0,0.4),\n",
    "    ylabel=\"\",\n",
    "    xticks=np.arange(0.2, 0.8, 0.1),\n",
    "    yticks=np.arange(0, 0.5, 0.1),\n",
    "    xlim=(0.2, 0.6),\n",
    ")\n",
    "\n",
    "rect = plt.Rectangle((0.2, 0.004), 0.4, 0.396, facecolor=(0.1, 0.1, 0.1, 0.05), edgecolor=\"grey\", linestyle=\"-\")\n",
    "ax1.add_patch(rect)\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles = list(handles) + [rect]\n",
    "labels = list(labels) + [\"Plot on the right\"]\n",
    "ax1.legend(handles, labels, title=\"Variant\")\n",
    "\n",
    "sns.move_legend(\n",
    "    ax1,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=[1.08, -0.32],\n",
    "    ncol=7\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
