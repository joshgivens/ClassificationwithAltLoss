{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class prior for positive: 0.011029\n",
      "class prior for negative: 0.988971\n"
     ]
    }
   ],
   "source": [
    "# Song Liu (song.liu@bristol.ac.uk), 01-06-2023\n",
    "# All rights reserved.\n",
    "\n",
    "# load csv files from archive folder\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import  *\n",
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('archive/Variant I.csv')\n",
    "# remove 'income', 'customer_age', 'employment_status' columns as they are protected\n",
    "\n",
    "customer_age = base.customer_age\n",
    "income = base.income\n",
    "employment_status = base.employment_status\n",
    "\n",
    "base = base.drop(columns=['income', 'customer_age', 'employment_status'])\n",
    "# convert categorical variables in 'payment_type' to integers\n",
    "base['payment_type'] = base['payment_type'].astype('category')\n",
    "base['housing_status'] = base['housing_status'].astype('category')\n",
    "base['source'] = base['source'].astype('category')\n",
    "base['device_os'] = base['device_os'].astype('category')\n",
    "\n",
    "cat_columns = base.select_dtypes(['category']).columns\n",
    "base[cat_columns] = base[cat_columns].apply(lambda x: x.cat.codes)\n",
    "base = base.to_numpy()\n",
    "\n",
    "# class prior \n",
    "prior = sum(base[:,0])/len(base)\n",
    "\n",
    "print('class prior for positive:', prior)\n",
    "print('class prior for negative:', 1-prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "customer_age_binary = (customer_age<=40).astype(int).to_numpy()\n",
    "customer_age_binary = np.array([customer_age_binary]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = np.hstack((base, customer_age_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a classifier (logistic regression with linear model) on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(pred, label):\n",
    "    fpr, tpr = [], []\n",
    "    tmax = max(pred)\n",
    "    tmin = min(pred)\n",
    "    for t in linspace(tmin, tmax, 100):\n",
    "        p = pred > t\n",
    "        tpr.append(TPR(p, label))\n",
    "        fpr.append(FPR(p, label))\n",
    "    return fpr, tpr\n",
    "\n",
    "def AUC(fpr, tpr):\n",
    "    return sum([(tpr[i]+tpr[i-1])*(fpr[i-1]-fpr[i])/2 for i in range(1, len(fpr))])\n",
    "\n",
    "# train a logistic regression using sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = base[:,1:]\n",
    "y = base[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, customer_age_binary, test_size=0.1, random_state=0)\n",
    "\n",
    "# # not weighted\n",
    "# clf0 = LogisticRegression(random_state=0, max_iter = 100).fit(X_train, y_train, sample_weight = None)\n",
    "# # weight the samples according to the class prior\n",
    "# weights = y_train*(1-prior) + (1-y_train)*prior\n",
    "# clf1 = LogisticRegression(random_state=0, max_iter = 100).fit(X_train, y_train, sample_weight = weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better than baseline classifiers. Now try AUC maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_constraints.fairbatch.FairBatchSampler import FairBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(-1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57323/3802244508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFairBatch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'eqopp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mtrainload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# trainload = torch.utils.data.DataLoader(dataset, batch_size = 10000, shuffle = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/lv/ClassificationwithAltLoss/fairness_constraints/fairbatch/FairBatchSampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, x_tensor, y_tensor, z_tensor, batch_size, alpha, target_fairness, replacement, seed)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (-1, 1)"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "# Let us maximize the AUC\n",
    "import torch # bring out the big gun\n",
    "\n",
    "def aucloss(pred, label):\n",
    "    if pred[label==1].shape[0] == 0: # you might end up with no positive samples\n",
    "        print('no positive samples!')\n",
    "    pos_pred = pred[label==1]\n",
    "    neg_pred = pred[label==0]\n",
    "    \n",
    "    # hinge loss\n",
    "    T = (neg_pred.T - pos_pred)\n",
    "    loss = torch.max(T, torch.zeros_like(T))/2\n",
    "    loss = torch.mean(loss, dim = 0)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "# same linear model\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc = torch.nn.Linear(d, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "X_tr = torch.tensor(X_train, dtype = torch.float32)\n",
    "y_tr = torch.tensor(y_train, dtype = torch.float32)\n",
    "z_tr = torch.tensor(z_train.squeeze(), dtype = torch.float32)\n",
    "X_te = torch.tensor(X_test, dtype = torch.float32)\n",
    "y_te = torch.tensor(y_test, dtype = torch.float32)\n",
    "z_te = torch.tensor(z_test.squeeze(), dtype = torch.float32)\n",
    "\n",
    "model = NN(X_train.shape[1])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_tr, y_tr, z_tr)\n",
    "\n",
    "\n",
    "sampler = FairBatch (model, X_tr, y_tr, z_tr, batch_size = 100, alpha = 0.005, target_fairness = 'eqopp', replacement = False, seed = 0)\n",
    "trainload = torch.utils.data.DataLoader (dataset, sampler=sampler, num_workers=0)\n",
    "# trainload = torch.utils.data.DataLoader(dataset, batch_size = 10000, shuffle = True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for x, y in trainload:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = aucloss(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pred_te = model(X_te).detach().numpy()[:,0]\n",
    "\n",
    "    fpr, tpr = roc(pred_te, y_test)\n",
    "    auc = AUC(fpr, tpr)\n",
    "    \n",
    "    pl.plot(fpr, tpr, label = 'epoch %d, AUC: %0.2f' % (epoch, auc))\n",
    "    pl.legend(loc = 'lower right')\n",
    "    display.display(pl.gcf())\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
